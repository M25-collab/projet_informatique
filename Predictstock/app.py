from flask import Flask, render_template, jsonify, request
import yfinance as yf
import time
import json
import os
import feedparser
import re
from apscheduler.schedulers.background import BackgroundScheduler
from datetime import datetime
import pytz
import csv
from flask import send_file
import subprocess
import threading
from flask import redirect, url_for

app = Flask(__name__)


"""def safe_get_history(ticker_str, period="6mo", interval="1wk"):
    retries = 5
    wait = 30
    for attempt in range(retries):
        try:
            ticker = yf.Ticker(ticker_str)
            hist = ticker.history(period=period, interval=interval)
            time.sleep(1)  # petite pause pour √™tre poli
            return hist
        except Exception as e:
            if "Too Many Requests" in str(e):
                print(f"‚ùå Rate limit hit for {ticker_str}. Waiting {wait} seconds before retry {attempt+1}...")
                time.sleep(wait)
                wait *= 2  # backoff exponentiel
            else:
                print(f"‚ùå Erreur r√©cup√©ration {ticker_str} : {e}")
                break
    return None"""






# Partie app.py : Stock Data

cache = {
    "data": None,
    "timestamp": 0
}
CACHE_DURATION = 180  # secondes
LOG_FILE = "stock_data_log.jsonl"

def get_stock_price(ticker_symbol):
    try:
        ticker = yf.Ticker(ticker_symbol)
        
        info = ticker.info
        price = info.get("currentPrice")
        return float(price) if price else None
    except Exception as e:
        print(f"Erreur r√©cup√©ration {ticker_symbol}: {e}")
        return None

def fetch_all_prices():
    danone = get_stock_price("BN.PA")
    loreal = get_stock_price("OR.PA")
    airfrance = get_stock_price("AF.PA")

    data = {
        "danone": [danone]*7 if danone else [0]*7,
        "loreal": [loreal]*7 if loreal else [0]*7,
        "airfrance": [airfrance]*7 if airfrance else [0]*7
    }

    log_data(data)
    return data

def log_data(data):
    try:
        log_entry = {
            "timestamp": time.strftime('%Y-%m-%d %H:%M:%S'),
            "data": data
        }
        with open(LOG_FILE, 'a') as f:
            f.write(json.dumps(log_entry) + '\n')
    except Exception as e:
        print(f"Erreur de log: {e}")

@app.route('/')
def accueil():
    return render_template('accueil.html')

@app.route('/utilisateurs')
def utilisateurs():
    return render_template('utilisateurs.html')

@app.route('/ressources')
def ressources():
    pdf_folder = os.path.join(app.static_folder, 'pdfs')
    fichiers = os.listdir(pdf_folder)

    fichiers_par_entreprise = {
        "danone": [],
        "loreal": [],
        "airfrance": []
    }

    for fichier in fichiers:
        lower = fichier.lower()
        if lower.startswith("danone"):
            fichiers_par_entreprise["danone"].append(fichier)
            time.sleep(1)
        elif lower.startswith("loreal") or lower.startswith("lor√©al"):
            fichiers_par_entreprise["loreal"].append(fichier)
            time.sleep(1)
        elif lower.startswith("airfrance"):
            fichiers_par_entreprise["airfrance"].append(fichier)
            time.sleep(1)

    for entreprise in fichiers_par_entreprise:
        fichiers_par_entreprise[entreprise].sort()
        time.sleep(1)

    return render_template("ressources.html", fichiers=fichiers_par_entreprise)

@app.route('/actualites')  
def actualites():
    return render_template('actualites.html')

CACHE_FILE = "cache_stock_data.json"  # Fichier pour cache persistant sur disque

# Charger cache depuis disque au d√©marrage (sinon vide)
def load_cache_from_file():
    if os.path.exists(CACHE_FILE):
        try:
            with open(CACHE_FILE, 'r') as f:
                cache_data = json.load(f)
                # V√©rifier que le timestamp est pr√©sent
                if "timestamp" in cache_data and "data" in cache_data:
                    return cache_data
        except Exception as e:
            print(f"Erreur chargement cache disque: {e}")
    return {"data": None, "timestamp": 0}

# Sauvegarder cache dans fichier
def save_cache_to_file(cache_data):
    try:
        with open(CACHE_FILE, 'w') as f:
            json.dump(cache_data, f)
    except Exception as e:
        print(f"Erreur sauvegarde cache disque: {e}")

# Initialisation cache m√©moire en important les donn√©es du fichier au d√©marrage
cache = load_cache_from_file()

CACHE_DURATION = 30  # secondes (tu peux augmenter)

"""@app.route('/api/stock-data')
def api_stock_data():
    now = time.time()

    # 1) V√©rifier si cache m√©moire est valide
    if cache["data"] and now - cache["timestamp"] < CACHE_DURATION:
        print("‚úÖ Donn√©es servies depuis le cache m√©moire")
        return jsonify(cache["data"])
    
    # 2) Sinon, essayer de charger cache depuis disque (persistant)
    cache_file_data = load_cache_from_file()
    if cache_file_data["data"] and now - cache_file_data["timestamp"] < CACHE_DURATION:
        # Mettre √† jour cache m√©moire depuis cache disque
        cache["data"] = cache_file_data["data"]
        cache["timestamp"] = cache_file_data["timestamp"]
        print("‚úÖ Donn√©es servies depuis le cache disque")
        return jsonify(cache["data"])

    # 3) Sinon, recharger depuis yfinance
    print("üîÑ Mise √† jour des donn√©es avec yfinance...")
    data = fetch_all_prices()

    # 4) Mettre √† jour cache m√©moire et disque
    cache["data"] = data
    cache["timestamp"] = now
    save_cache_to_file(cache)

    return jsonify(data)"""

# --- FIN modification cache persistante ---

# --- A COMMENTER ou supprimer si tu veux, car remplac√© par le cache persist√©

@app.route('/api/stock-data')
def api_stock_data():
    now = time.time()
    if cache["data"] and now - cache["timestamp"] < CACHE_DURATION:
        print("‚úÖ Donn√©es servies depuis le cache")
        return jsonify(cache["data"])
    
    print("üîÑ Mise √† jour des donn√©es avec yfinance...")
    data = fetch_all_prices()
    cache["data"] = data
    cache["timestamp"] = now
    return jsonify(data)


@app.route('/api/historical-stock-data')
def api_historical_stock_data():
    try:
        period = "6mo"
        interval = "1wk"

        tickers = {
            "danone": "BN.PA",
            "loreal": "OR.PA",
            "airfrance": "AF.PA"
        }

        # --- Modification: centraliser l'appel yfinance ici ---
        ticker_symbols_str = " ".join(tickers.values())  # "BN.PA OR.PA AF.PA"
        tickers_obj = yf.Tickers(ticker_symbols_str)

        data = {}
        for name, symbol in tickers.items():
            # Utilisation du ticker centralis√©
            # ATTENTION: yfinance ne garantit pas toujours la fiabilit√© sur plusieurs tickers
            ticker = yf.Ticker(symbol)
            hist = ticker.history(period=period, interval=interval)
            time.sleep(1)  # diminuer par rapport √† avant (45s)
            if hist is None or hist.empty:
                data[name] = {"dates": [], "values": []}
            else:
                data[name] = {
                    "dates": hist.index.strftime('%Y-%m-%d').tolist(),
                    "values": hist["Close"].fillna(0).tolist()
                }

        return jsonify(data)
    except Exception as e:
        print("Erreur donn√©es historiques :", e)
        return jsonify({"error": str(e)}), 500
    

# Partie actu_geo.py : Actualit√©s g√©opolitiques

# Liste de mots cl√©s pour l'√©valuation des articles
mots_negatifs = [
    "guerre", "conflit", "r√©cession", "inflation", "crise √©conomique", "panne", "gr√®ve", "licenciements",
    "r√©duction des co√ªts", "r√©duction des effectifs", "fermeture d'usine", "d√©t√©rioration", "incertitude",
    "perte", "faillite", "scandale", "retard", "suspension", "d√©p√¥t de bilan", "ch√¥mage", "probl√®mes de production",
    "plan de restructuration", "mauvais r√©sultats", "abandon de projet", "pollution", "manque de financement",
    "concurrence accrue", "instabilit√©", "d√©clin", "crise de confiance", "ralentissement", "embargo", "d√©sastre",
    "controverse", "disruption", "contraction", "scandale fiscal", "non-conformit√©", "mauvaise performance",
    "r√©trogradation", "mauvaise gestion", "non-rentabilit√©", "retards significatifs", "dissolution de partenariat",
    "impact environnemental n√©gatif", "d√©mission du PDG", "d√©part d'un cadre cl√©", "appels √† la liquidation",
    "perte de contrats majeurs", "d√©rives √©thiques"
]

mots_neutres = [
    "projet", "rapport", "investissement", "expansion", "nouvelle technologie", "r√©vision", "progr√®s", "lancement",
    "r√©flexion", "collaboration", "nouveau partenariat", "recrutement", "diversification", "initiative", "fusion",
    "acquisition", "pr√©sentation", "discussion", "objectif", "recherche", "protocole", "test", "investisseur",
    "consensus", "march√©", "indicateur", "analyse", "partenariat strat√©gique", "√©valuation"
]

mots_positifs = [
    "bons r√©sultats", "croissance", "succ√®s", "innovation", "r√©volutionnaire", "expansion", "rentabilit√©",
    "part de march√©", "leadership", "nouveau produit", "collaboration fructueuse", "forte demande",
    "augmentation des b√©n√©fices", "nouveaux contrats", "augmentation des ventes", "augmentation de la production",
    "bonnes perspectives", "r√©compense", "r√©alisation", "partenariat strat√©gique", "recrutement renforc√©",
    "confiance", "durabilit√©", "prise de leadership", "investissement fructueux", "valeur ajout√©e",
    "expansion internationale", "d√©veloppement", "r√©ussite", "excellence", "soutien gouvernemental",
    "am√©lioration", "performance exceptionnelle", "progr√®s exceptionnels", "croissance exponentielle",
    "innovation de rupture", "leadership sur le march√©", "partenariat gagnant-gagnant", "prise de part de march√©",
    "recrutement d'experts", "augmentation de la rentabilit√©", "r√©cup√©ration post-crise"
]

# Coefficients de pond√©ration pour les mots positifs, n√©gatifs et la variation du cours
coefficients = {
    'mots_positifs': 1.0,
    'mots_negatifs': -1.0,
    'variation_cours': 0.5
}

# Variable globale pour stocker les articles √©valu√©s
evaluated_articles = []

# Charger les coefficients depuis un fichier JSON
def load_coefficients():
    global coefficients
    try:
        with open('coefficients.json', 'r') as file:
            coefficients = json.load(file)
    except FileNotFoundError:
        pass

# Sauvegarder les coefficients dans un fichier JSON
def save_coefficients():
    with open('coefficients.json', 'w') as file:
        json.dump(coefficients, file)

# √âvaluer le texte en fonction des mots cl√©s et des coefficients de pond√©ration
def evaluate_text(text):
    score = 0
    words = re.findall(r'\b\w+\b', text.lower())
    for word in words:
        if word in mots_positifs:
            score += coefficients['mots_positifs']
        elif word in mots_negatifs:
            score += coefficients['mots_negatifs']
    return score

# R√©cup√©rer les articles g√©opolitiques depuis un flux RSS
def get_rss_articles():
    url = 'https://www.lemonde.fr/rss/une.xml'
    feed = feedparser.parse(url)
    articles = []
    for entry in feed.entries:
        articles.append({
            'title': entry.title,
            'description': entry.description,
            'url': entry.link
        })
    return articles

# Scraper les articles et les √©valuer
def scrape_articles():
    global evaluated_articles
    articles = get_rss_articles()
    evaluated_articles = []
    for article in articles:
        titre = article['title']
        description = article['description']
        full_text = titre + " " + (description or "")

        # √âvaluer le texte de l'article
        score = evaluate_text(full_text)
        evaluated_articles.append({
            'titre': titre,
            'description': description,
            'url': article['url'],
            'score': score
        })

# Endpoint pour obtenir les 3 premiers articles √©valu√©s
@app.route('/actu', methods=['GET'])
def get_actu():
    top_articles = sorted(evaluated_articles, key=lambda x: x['score'], reverse=True)[:3]
    return jsonify(top_articles)

# Planifier la mise √† jour des articles toutes les 24 heures √† 6h (heure fran√ßaise)
def schedule_article_update():
    scheduler = BackgroundScheduler()
    paris_tz = pytz.timezone('Europe/Paris')
    scheduler.add_job(scrape_articles, 'cron', hour=6, timezone=paris_tz)
    scheduler.start()



#ici pour l'IA
def export_historique_csv(company, filename='historique.csv'):
    """
    Exporte les donn√©es historiques pour une seule entreprise
    dans un CSV au format : jour,prix,score
    o√π 'jour' est un indice 0,1,2..., 'prix' est le cours de cl√¥ture,
    et 'score' le score d'actualit√© (ici on mettra 0 ou un score moyen).
    """
    # R√©cup√©rer les donn√©es historiques (comme dans ton endpoint)
    period = "6mo"; interval = "1wk"
    #hist = yf.Ticker({"danone":"BN.PA","loreal":"OR.PA","airfrance":"AF.PA"}[company]) \
    #       .history(period=period, interval=interval)
    symbol = {"danone":"BN.PA", "loreal":"OR.PA", "airfrance":"AF.PA"}[company]
    time.sleep(1)
    ticker = yf.Ticker(symbol)
    hist = ticker.history(period=period, interval=interval)
    if hist is None or hist.empty:
        print(f"‚ùå Impossible de r√©cup√©rer l'historique pour {company}.")
        return
    time.sleep(1)
    dates   = hist.index.strftime('%Y-%m-%d').tolist()
    closes  = hist["Close"].fillna(0).tolist()

    # Ici on met un score d'actualit√© par d√©faut √† 0,
    # ou tu peux calculer une moyenne de evaluated_articles si tu veux.
    # Par exemple :
    avg_score = 0
    if evaluated_articles:
        avg_score = sum(a['score'] for a in evaluated_articles) / len(evaluated_articles)

    # √âcriture du CSV
    with open(filename, 'w', newline='', encoding='utf-8') as f:
        writer = csv.writer(f)
        writer.writerow(['jour','prix','score'])
        for i, price in enumerate(closes):
            writer.writerow([i, price, avg_score])
    print(f"‚Üí {filename} g√©n√©r√© pour {company}")



#pour optimiser les fichier csv
@app.route('/api/export-csv/<company>', methods=['GET'])
def export_csv(company):
    # D√©terminer le nom du fichier en fonction de l'entreprise
    filename = f'historique_{company}.csv'
    
    # G√©n√©rer le fichier CSV si n√©cessaire
    export_historique_csv(company, filename)
    time.sleep(1)
    
    # Renvoi du fichier CSV
    return send_file(filename, as_attachment=True)


@app.route('/api/predict/<company>', methods=['GET'])
def predict(company):
    filename = f'historique_{company}.csv'
    export_historique_csv(company, filename)
    lancer_ia(company)
    return jsonify({"message": f"Pr√©diction lanc√©e pour {company}."})


def lancer_ia(company):
    """
    Lance le programme C de pr√©diction boursi√®re pour une entreprise donn√©e.
    Il utilise le fichier CSV g√©n√©r√© pr√©c√©demment, et produit un fichier de sortie prediction_<entreprise>.txt
    """
    # G√©n√®re les noms des fichiers bas√©s sur le nom de l'entreprise
    csv_hist = f'historique_{company}.csv'  # Fichier CSV historique
    txt_corr = f'prediction_{company}_corr.txt'  # Fichier de sortie pour les r√©sultats
    txt_init = f'prediction_{company}_init.txt'  # Fichier de sortie initial
    exe_name = 'mon_ia_c.exe'  # Nom de l'ex√©cutable C

    # V√©rifie si l'ex√©cutable existe
    if not os.path.isfile(exe_name):
        print(f"‚ùå L'ex√©cutable '{exe_name}' est introuvable dans le r√©pertoire actuel.")
        return

    # V√©rifie si le fichier CSV existe
    if not os.path.isfile(csv_hist):
        print(f"‚ùå Le fichier '{csv_hist}' est introuvable.")
        return

    try:
        # Appel du programme IA via subprocess
        result = subprocess.run(
            [exe_name, csv_hist, txt_init, txt_corr],  # Passe les fichiers appropri√©s
            stdout=subprocess.PIPE,  # Capture la sortie standard
            stderr=subprocess.PIPE,  # Capture les erreurs
            check=True  # L√®ve une exception en cas d'erreur
        )
        print(f"‚úÖ Pr√©diction g√©n√©r√©e pour {company} ‚Üí {txt_corr}")
    except subprocess.CalledProcessError as e:
        print(f"‚ùå Erreur lors de l'ex√©cution de l'IA : {e.stderr.decode()}")
    except FileNotFoundError as e:
        print(f"‚ùå Erreur : {e}")

# Il faut ATTENDRE vraiment quelques secondes voir une bonne minute piti√© sinon trop de demande, piti√©, si fonctionne pas enlever VPN
@app.route('/api/prediction-data', methods=['GET'])
def prediction_data():
    companies = ['danone', 'loreal', 'airfrance']
    prediction_results = {}
    for company in companies:
        prediction_file = f"prediction_{company}_corr.txt"
        try:
            with open(f'ia/{company}_txt_corr.txt', 'r') as file:
                predictions = file.readlines()
                prediction = float(predictions[-1].strip())
                prediction_results[company] = [prediction]
        except FileNotFoundError:
            print(f"Fichier non trouv√© : {prediction_file}")
            prediction_results[company] = [0]
    return {"data": prediction_results, "timestamp": time.time()}




#Routes vers les nouvelles pages

@app.route('/mentions')
def mentions():
    return render_template('mentions.html')

@app.route('/contact')
def contact():
    return render_template('contact.html')

@app.route('/reseaux')
def reseaux():
    return render_template('reseaux.html')

@app.route('/faq')
def faq():
    return render_template('faq.html')

@app.route('/support')
def support():
    return render_template('support.html')


@app.route('/admin/run_exports')
def run_exports():
    def background_task():
        for company in ['danone', 'loreal', 'airfrance']:
            export_historique_csv(company)
            lancer_ia(company)


    thread = threading.Thread(target=background_task)
    thread.start()

    return jsonify({"status": "Exportation et IA lanc√©s en arri√®re-plan"})



# D√©marrage de l'application
if __name__ == '__main__':
    # 1) Charger les coefficients et r√©cup√©rer les articles
    load_coefficients()
    scrape_articles()
    schedule_article_update()

    # 2) G√©n√©rer les CSV historiques + lancer l‚ÄôIA C pour chaque entreprise
    """for company in ['danone', 'loreal', 'airfrance']:
        # a) CSV prix + score
        export_historique_csv(company, f'historique_{company}.csv')
        
        # b) Appel √† l‚ÄôIA C => cr√©ation de prediction_<company>.txt
        lancer_ia(company)"""
        

    # 3) Lancer le serveur Flask (une seule fois !)
    app.run(debug=True)





